<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Changqun Xia</title>
  
  <meta name="author" content="Changqun Xia">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Changqun Xia</name>
              </p>
              <p>Currently I am  an associate professor in Peng Cheng Laboratory (PCL), Shenzhen, China</a>.
              </p>
              <p>
               I have received the Ph.D degree from the State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, advised by <a href="http://cvteam.net">Prof. Jia Li</a>. I am the author of refereed journals and conferences such as TPAMI, TIP, CVPR and ICCV. Email: xiachq@pcl.ac.cn.
              </p>
              <p style="text-align:center">
                <a href="mailto:xiachq@pcl.ac.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=PJG_UUUAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
				<a href="http://cvteam.net">CVTeam</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/portrait2.jpg"><img style="width:60%;max-width:60%" alt="profile photo" src="images/portrait2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I study computer vision and machine learning. My research lies much in image and video processing with learning and optimization methods.
              </p>
              <p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>















         <tr>
			 <td style="padding:20px;width:25%;vertical-align:middle">
			   <div class="one">
                 <img src='images/2023_TIP_BBRF.png' width="160" height="120">
               </div>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>Boosting Broader Receptive Fields for Salient Object Detection</papertitle>
               <br>
               Mingcan Ma#,
               <strong>Changqun Xia#<sup></sup></strong>,
	           Chenxi Xie,
			   Xiaowu Chen,
               Jia Li
               <br>
                 <em>TIP</em>, 2023
               <br>
	           <a href="data/2023_TIP_BBRF.pdf">paper</a> 
	           /  
	           <a href="https://github.com/iCVTEAM/BBRF-TIP">code</a>
               <p></p>
               <p>We explore that it is difficult to segment large or small-scale objects due to their asymmetric segmentation requirements. We deconstruct the role of receptive fields in SOD and introduce a Bilateral Extreme Stripping encoder based on the simplified vision transformer and the lightweight CNN for the broader receptive fields.
             </td>
           </tr>  
			


         <tr>
			 <td style="padding:20px;width:25%;vertical-align:middle">
			   <div class="one">
                 <img src='images/2022_TMM_360.png' width="160" height="120">
               </div>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>View-aware Salient Object Detection for 360 Omnidirectional Image</papertitle>
               <br>
               Junjie Wu,
               <strong>Changqun Xia*</strong>,
	           Tianshu Yu,
               Jia Li
               <br>
                 <em>TMM</em>, 2022
               <br>
	           <a href="data/2022_TMM_360.pdf">paper</a> 
	           /  
	           <a href="https://github.com/iCVTEAM/ODI-SOD">dataset</a>
               <p></p>
               <p>We construct a 360 omnidirectional image-based SOD dataset, namely ODI-SOD. It has object-level pixelwise annotations on ERP images and is the largest dataset for 360 ISOD by far to our best knowledge. Moreover, we propose a view-aware salient object detection method for 360 ODIs.  
             </td>
           </tr>  
            

           <tr>
			 <td style="padding:20px;width:25%;vertical-align:middle">
			   <div class="one">
                 <img src='images/2022_CVPR_PGNet.png' width="160" height="120">
               </div>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>Pyramid Grafting Network for One-Stage High Resolution Saliency Detection</papertitle>
               <br>
               Chenxi Xie,
               <strong>Changqun Xia*</strong>,
	           Mingcan Ma,
               Zhirui Zhao,
			   Xiaowu Chen,
			   Jia Li
               <br>
                 <em>CVPR</em>, 2022
               <br>
	           <a href="data/2022_CVPR_PGNet.pdf">paper</a> 
	           / 
	           <a href="https://github.com/iCVTEAM/PGNet">Project</a>
               <p></p>
               <p>We contribute a new Ultra-High-Resolution Saliency Detection dataset UHRSD, containing 5920 images at 4K-8K resolutions. To our knowledge, it is the largest dataset in both quantity and resolution for high-resolution SOD task. We propose a novel one-stage framework called Pyramid Grafting Network.
             </td>
           </tr>  



           <tr>
			 <td style="padding:20px;width:25%;vertical-align:middle">
			   <div class="one">
                 <img src='images/2021_TIP_PMSS.png' width="160" height="120">
               </div>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>Salient Object Detection with Purificatory Mechanism and Structural Similarity Loss</papertitle>
               <br>
               Jia Li,
			   Jinming Su,
               <strong>Changqun Xia*</strong>,
	           Mingcan Ma,
			   Yonghong Tian
               <br>
                 <em>TIP</em>, 2021
               <br>
	           <a href="data/2021_TIP_PMSS.pdf">paper</a>
			   / 
	           <a href="https://github.com/Jinming-Su/PurNet">Project</a>
               <p></p>
               <p>We rethink the two difficulties that hinder the development of salient object detection, which consists of indistinguishable regions and complex structures. To solve these two issues, we propose the purificatory network with structural similarity loss. 
             </td>
           </tr>  


           <tr>
			 <td style="padding:20px;width:25%;vertical-align:middle">
			   <div class="one">
                 <img src='images/2021_AAAI_PFS.png' width="160" height="160">
               </div>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>Pyramidal Feature Shrinking for Salient Object Detection</papertitle>
               <br>
               Mingcan Ma,
               <strong>Changqun Xia*</strong>,
	           Jia Li
               <br>
                 <em>AAAI</em>, 2021
               <br>
	           <a href="data/2021_AAAI_PFS.pdf">paper</a> 
	           / 
	           <a href="https://github.com/iCVTEAM/PFSNet">project</a>
               <p></p>
               <p>Existing methods usually aggregate the low-level features containing details and the high-level features containing semantics over a large span, which introduces noise into the aggregated features and generates inaccurate saliency maps. In this paper, we propose a pyramidal feature shrinking network. 
             </td>
           </tr>  


           <tr>
			 <td style="padding:20px;width:25%;vertical-align:middle">
			   <div class="one">
                 <img src='images/2021_MM_CTD.png' width="160" height="160">
               </div>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>Complementary Trilateral Decoder for Fast and Accurate Salient Object Detection</papertitle>
               <br>
               Zhirui Zhao,
               <strong>Changqun Xia*</strong>,
			   Chenxi Xie,
	           Jia Li
               <br>
				 <em>ACM MM</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
               <br>
	           <a href="data/2021_MM_CTD.pdf">paper</a> 
	           / 
	           <a href="https://github.com/iCVTEAM/CTDNet">code</a>
			   
               <p></p>
               <p>Most of existing SOD methods focus more on performance than efficiency. Besides, the U-shape structure exists some drawbacks and there is still a lot of room for improvement. Therefore, we propose a novel framework to treat semantic context, spatial detail and boundary information separately in the decoder part. 
             </td>
           </tr>  


           <tr>
			 <td style="padding:20px;width:25%;vertical-align:middle">
			   <div class="one">
                 <img src='images/2021_ICME_Task.png' width="160" height="160">
               </div>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>Exploring Driving-aware Salient Object Detection via Knowledge Transfer</papertitle>
               <br>
               Jinming Su,
               <strong>Changqun Xia*</strong>,
	           Jia Li
               <br>
                 <em>ICME</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
               <br>
	           <a href="data/2021_ICME_Task.pdf">paper</a> 
               <p></p>
               <p>Task-aware SOD has hardly been studied due to the lack of task-specific datasets. In this paper, we construct a driving task-oriented dataset where pixel-level masks of salient objects have been  annotated. We proposed a baseline model for the driving task-aware SOD via a knowledge transfer convolutional neural network.
             </td>
           </tr>  



           <tr>
			 <td style="padding:20px;width:25%;vertical-align:middle">
			   <div class="one">
                 <img src='images/2020_JSTSP_DSS.png' width="160" height="160">
               </div>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>Distortion-adaptive Salient Object Detection in 360 Omnidirectional Images</papertitle>
               <br>
               Jia Li,
			   Jinming Su,
               <strong>Changqun Xia*</strong>,
	           Yonghong Tian
               <br>
                 <em>JSTSP</em>, 2020 
               <br>
	           <a href="data/2020_JSTSP_DSS.pdf">paper</a>  
	           /
	           <a href="http://cvteam.net/projects/JSTSP20_DDS/DDS.html">project</a>    
               <p></p>
               <p>SOD on 360 omnidirectional images is less studied owing to the lack of datasets with pixel-level annotations. Toward this end, this paper proposes a 360 image-based SOD dataset that contains 500 high-resolution equirectangular images. We proposes a baseline model for SOD on equirectangular images to deal with the distortion caused by the equirectangular projection.
             </td>
           </tr>  




           <tr>
			 <td style="padding:20px;width:25%;vertical-align:middle">
			   <div class="one">
                 <img src='images/2019_ICCV_BANet.png' width="160" height="160">
               </div>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>Selectivity or Invariance: Boundary-aware Salient Object Detection</papertitle>
               <br>
			   Jinming Su,
			   Jia Li*,
			   Yu Zhang,
               <strong>Changqun Xia</strong>,
	           Yonghong Tian
               <br>
                 <em>ICCV</em>, 2019 
               <br>
	           <a href="data/2019_ICCV_BANet.pdf">paper</a>  
	           /
	           <a href="http://cvteam.net/projects/ICCV19-SOD/BANet.html">project</a>    
               <p></p>
               <p>Typically, a salient object detection (SOD) model faces opposite requirements in processing object interiors and boundaries. To address this selectivityinvariance dilemma, we propose a novel boundary-aware network with successive dilation for image-based SOD.
             </td>
           </tr>  

             
   

           <tr>
			 <td style="padding:20px;width:25%;vertical-align:middle">
			   <div class="one">
                 <img src='images/2018_TPAMI_SOS.png' width="160" height="160">
               </div>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>Semantic Object Segmentation in Tagged Videos via Detection</papertitle>
               <br>
			   Yu Zhang,
			   Xiaowu Chen,
			   Jia Li,
			   Chen Wang,
			   <strong>Changqun Xia</strong>,
			   Jun Li
               <br>
                 <em>TPAMI</em>, 2018
               <br>
	           <a href="data/2018_TPAMI_SOS.pdf">paper</a>    
               <p></p>
               <p>Extended version of CVPR 2015 with improved network flow solver and object shape prior.
             </td>
           </tr> 



           <tr>
			 <td style="padding:20px;width:25%;vertical-align:middle">
			   <div class="one">
                 <img src='images/2018_TIP_SSA.png' width="160" height="160">
               </div>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>A Benchmark Dataset and Saliency-guided Stacked Autoencoders for Video-based Salient Object Detection</papertitle>
               <br>
               Jia Li,
               <strong>Changqun Xia</strong>,
	           Xiaowu Chen
               <br>
                 <em>TIP</em>, 2018
               <br>
	           <a href="data/2018_TIP_SSA.pdf">paper</a> 
			   /
	           <a href="http://cvteam.buaa.edu.cn/projects/TIP18-VOS/VOS.htmll">project</a>

               <p></p>
               <p>video-based SOD is much less explored due to the lack of large-scale video datasets within which salient objects are unambiguously defined and annotated. Toward this end, this paper proposes a video-based SOD dataset that consists of 200 videos. Based on this dataset, this paper proposes an unsupervised baseline approach for video-based SOD by using saliency-guided stacked autoencoders. 
             </td>
           </tr>  



		     <tr>
			 <td style="padding:20px;width:25%;vertical-align:middle">
			   <div class="one">
                 <img src='images/2017_CVPR_ELE.png' width="160" height="160">
               </div>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>What is and What is not a Salient Object? Learning Salient Object Detector by Ensembling Linear Exemplar Regressors</papertitle>
               <br>
               <strong>Changqun Xia</strong>,
			   Jia Li,
	           Xiaowu Chen,
			   Anlin Zheng,
			   Yu Zhang
               <br>
                 <em>CVPR</em>, 2017
               <br>
	           <a href="data/2017_CVPR_ELE.pdf">paper</a> 
			   /
	           <a href="http://cvteam.net/projects/CVPR17-ELE/ELE.html">project</a>

               <p></p>
               <p>Finding what is and what is not a salient object can be helpful in developing better features and models in salient object detection (SOD). In this paper, we investigate the images that are selected and discarded in constructing a new SOD dataset and find that many similar candidates, complex shape and low objectness are three main attributes of many non-salient objects. Then we propose a novel salient object detector by ensembling linear exemplar regressors. 
             </td>
           </tr>  
		   


           <tr>
			 <td style="padding:20px;width:25%;vertical-align:middle">
			   <div class="one">
                 <img src='images/2015_CVPR_SOS.png' width="160" height="160">
               </div>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>Semantic Object Segmentation via Detection in Weakly Labeled Video</papertitle>
               <br>
               Yu Zhang,
			   Xiaowu Chen,
			   Jia Li,
			   Chen Wang,
               <strong>Changqun Xia</strong>,
               <br>
                 <em>CVPR</em>, 2015
               <br>
	           <a href="data/2015_CVPR_SOS.pdf">paper</a> 

               <p></p>
               <p>Weak object detectors can generate strong video object segmentation results via joint inference with a quadratic network flow model.
             </td>
           </tr> 


 


           <tr>
			 <td style="padding:20px;width:25%;vertical-align:middle">
			   <div class="one">
                 <img src='images/2015_ICCV_Metric.png' width="160" height="160">
               </div>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>A Data-driven Metric for Comprehensive Evaluation of Saliency Models</papertitle>
               <br>
               Jia Li,
               <strong>Changqun Xia</strong>,
	           Yafei Song,
			   Shu Fang,
			   Xiaowu Chen
               <br>
                 <em>ICCV</em>, 2015
               <br>
	           <a href="data/2015_ICCV_Metric.pdf">paper</a> 
			   /
	           <a href="data/2015_ICCV_Video.mp4">Video</a>
               /
	           <a href="data/2015_ICCV_Data.rar">Data</a>
               /
	           <a href="data/2015_ICCV_metricCNN.rar">Code</a>/

               <p></p>
               <p>Existing metrics, which are often heuristically designed, may draw conflict conclusions in comparing saliency models. As a consequence, it becomes somehow confusing on the selection of metrics in comparing new models with state-of-the-arts. To address this problem, we propose a data-driven metric for comprehensive evaluation of saliency models. 
             </td>
           </tr>  



           <tr>
			 <td style="padding:20px;width:25%;vertical-align:middle">
			   <div class="one">
                 <img src='images/2015_ICVRV_AdaTemp.png' width="160" height="160">
               </div>
             </td>
             <td style="padding:20px;width:75%;vertical-align:middle">
                 <papertitle>Adaptive Template for Parsing Object of Indoor Scene Image</papertitle>
               <br>
               <strong>Changqun Xia</strong>,
			   Jie Xu,
			   Qing Li,
			   Yu Zhang,
			   Jia Li,
			   Xiaowu Chen
               <br>
                 <em>ICVRV</em>, 2015
               <br>
	           <a href="data/2015_ICVRV_AdaTemp.pdf">paper</a> 
               <p></p>
               <p>We propose an adaptive template for semantic labeling of indoor scene objects and estimating their oriented bounding facets (OBFs). The proposed adaptive template encodes prior geometric information of objects based on statistics of the training images. 
             </td>
           </tr>  



        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Much thanks to <a href="https://jonbarron.info/">Jon Barron</a> for sharing this template.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
